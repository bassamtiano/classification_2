{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import times_result\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-4.6.4-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: requests[socks] in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (from gdown) (2.28.1)\n",
      "Requirement already satisfied: filelock in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (from gdown) (3.8.0)\n",
      "Requirement already satisfied: tqdm in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (from gdown) (4.64.1)\n",
      "Requirement already satisfied: six in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (from gdown) (4.11.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (from requests[socks]->gdown) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (from requests[socks]->gdown) (2022.9.24)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Installing collected packages: gdown\n",
      "Successfully installed gdown-4.6.4\n",
      "Retrieving folder list\n",
      "Processing file 17cyEALCv23BSOnHHlhWw0Uiu_dTw4n5_ Bert Classification.ipynb\n",
      "Processing file 1eKwovBgJiT7QyJBOoZAx-8tpR50BIrhX test.pt\n",
      "Processing file 1tS306bxHFwyjVtsLIkIuK2fgWLErMqW6 testing.res\n",
      "Processing file 1SaOuoYAX1UUUNIjDclWSVAZOlyUWFlIi train.pt\n",
      "Processing file 1Bu-b6h5mbJcbt00dAs7Nwk7JqEhdoSiM training.res\n",
      "Processing file 11U-O-t8K2_cGbsB3h0WpfYXAYBcXSPZQ valid.pt\n",
      "Retrieving folder list completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=17cyEALCv23BSOnHHlhWw0Uiu_dTw4n5_\n",
      "To: /mnt/h/experiments/research_group/classification_2/classification/Bert Classification.ipynb\n",
      "100%|██████████████████████████████████████| 12.4k/12.4k [00:00<00:00, 33.1MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1eKwovBgJiT7QyJBOoZAx-8tpR50BIrhX\n",
      "To: /mnt/h/experiments/research_group/classification_2/classification/test.pt\n",
      "100%|██████████████████████████████████████| 6.41M/6.41M [00:02<00:00, 2.67MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1tS306bxHFwyjVtsLIkIuK2fgWLErMqW6\n",
      "To: /mnt/h/experiments/research_group/classification_2/classification/testing.res\n",
      "100%|██████████████████████████████████████| 3.37M/3.37M [00:01<00:00, 2.58MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1SaOuoYAX1UUUNIjDclWSVAZOlyUWFlIi\n",
      "To: /mnt/h/experiments/research_group/classification_2/classification/train.pt\n",
      "100%|██████████████████████████████████████| 15.0M/15.0M [00:07<00:00, 1.93MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Bu-b6h5mbJcbt00dAs7Nwk7JqEhdoSiM\n",
      "To: /mnt/h/experiments/research_group/classification_2/classification/training.res\n",
      "100%|██████████████████████████████████████| 7.87M/7.87M [00:02<00:00, 2.72MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=11U-O-t8K2_cGbsB3h0WpfYXAYBcXSPZQ\n",
      "To: /mnt/h/experiments/research_group/classification_2/classification/valid.pt\n",
      "100%|██████████████████████████████████████| 15.0M/15.0M [00:06<00:00, 2.18MB/s]\n",
      "Download completed\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --no-cache-dir gdown\n",
    "\n",
    "!gdown --folder https://drive.google.com/drive/folders/1ZRJ3Z16RuOYD8tanp27aqOC53fCftG_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Liputan6.com, London - Lee Dixon khawatir Arse...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Liputan6.com, Jakarta - Kasus dugaan penganiay...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Liputan6.com, Jakarta Menanggapi aksi eks peke...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Liputan6.com, Medan - Sebanyak 81 kendaraan 4x...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Liputan6.com, Jakarta Indonesia akan melawan T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6122</th>\n",
       "      <td>Liputan6.com, Jakarta Badan Pusat Statistik (B...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6123</th>\n",
       "      <td>Liputan6.com, Jakarta - PT Waskita Beton Preca...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6124</th>\n",
       "      <td>Liputan6SCTV, Bengkalis - Panglima TNI Marseka...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6125</th>\n",
       "      <td>Liputan6.com, Jakarta - Tahapan wawancara kerj...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6126</th>\n",
       "      <td>Jakarta - Timnas Indonesia U-22 bersua Vietnam...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6127 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  label\n",
       "0     Liputan6.com, London - Lee Dixon khawatir Arse...      0\n",
       "1     Liputan6.com, Jakarta - Kasus dugaan penganiay...      1\n",
       "2     Liputan6.com, Jakarta Menanggapi aksi eks peke...      2\n",
       "3     Liputan6.com, Medan - Sebanyak 81 kendaraan 4x...      0\n",
       "4     Liputan6.com, Jakarta Indonesia akan melawan T...      0\n",
       "...                                                 ...    ...\n",
       "6122  Liputan6.com, Jakarta Badan Pusat Statistik (B...      2\n",
       "6123  Liputan6.com, Jakarta - PT Waskita Beton Preca...      2\n",
       "6124  Liputan6SCTV, Bengkalis - Panglima TNI Marseka...      1\n",
       "6125  Liputan6.com, Jakarta - Tahapan wawancara kerj...      2\n",
       "6126  Jakarta - Timnas Indonesia U-22 bersua Vietnam...      0\n",
       "\n",
       "[6127 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {\n",
    "    'bola': 0,\n",
    "    'news': 1,\n",
    "    'bisnis': 2,\n",
    "    'tekno': 3,\n",
    "    'otomotif': 4\n",
    "}\n",
    "\n",
    "# def perhitungan(nilai):\n",
    "#   return nilai * 2\n",
    "\n",
    "def load_data():\n",
    "  with open(\"dataset/training.res\", \"rb\") as tdr:\n",
    "    train_pkl = pickle.load(tdr)\n",
    "    train = pd.DataFrame({'title': train_pkl[0], 'label': train_pkl[1]})\n",
    "  \n",
    "  with open(\"dataset/testing.res\", \"rb\") as tsdr:\n",
    "    test_pkl = pickle.load(tsdr)\n",
    "    test = pd.DataFrame({'title': test_pkl[0], 'label': test_pkl[1]})\n",
    "  \n",
    "  train.label = train.label.map(label2id)\n",
    "  test[\"label\"] = test.label.map(label2id)\n",
    "\n",
    "  # train[\"hitung\"] = train['label'].apply(lambda x: perhitungan(x))\n",
    "\n",
    "  return train, test\n",
    "\n",
    "train, test = load_data()\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Sastrawi in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install Sastrawi\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Membersihkan String dari Karakter yang tidak diinginkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_str(string):\n",
    "    string = string.lower()\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\-`]\", \" \", string)\n",
    "    # sebelum = hari ini mendung? | setelah = hari ini mendung ?\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\!\", \" \\! \", string)\n",
    "    \n",
    "    # sebelum = ayam,nasi,sambel | setelah ayam , nasi , sambel\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    \n",
    "    # menghilangkan whitespace / spasi berlebih\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    \n",
    "    # menghapus \\n / next line\n",
    "    string = re.sub(r\"\\n\", \"\", string)\n",
    "    \n",
    "    # menghapus \\n\\t / next line + tab\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    \n",
    "    string = string.strip()\n",
    "    \n",
    "    return stemmer.stem(string)\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "\n",
    "Merubah kata menjadi ID menggunakan pre-trained model BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (4.26.1)\n",
      "Requirement already satisfied: requests in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (from transformers) (2022.9.13)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (from transformers) (0.12.0)\n",
      "Requirement already satisfied: filelock in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (from transformers) (1.23.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: tqdm in /home/bassamtiano/anaconda3/envs/text/lib/python3.10/site-packages (4.64.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('indolem/indobert-base-uncased')\n",
    "\n",
    "max_sentence_length = 100\n",
    "\n",
    "def arange_data(data, type):\n",
    "    x_input_ids, y = [], []\n",
    "    \n",
    "    for i_baris, kalimat in enumerate(tqdm(data.values.tolist())):\n",
    "        judul = clean_str(kalimat[0])\n",
    "        label = kalimat[1]\n",
    "        \n",
    "        judul_ids = tokenizer(\n",
    "            text = judul,\n",
    "            max_length = max_sentence_length,\n",
    "            padding = 'max_length',\n",
    "            truncation = True\n",
    "        )[\"input_ids\"]\n",
    "        \n",
    "        y_label = [0] * len(label2id)\n",
    "        y_label[int(label)] = 1\n",
    "\n",
    "        x_input_ids.append(judul_ids)\n",
    "        y.append(y_label)\n",
    "        \n",
    "        # if i_baris > 10:\n",
    "        #     break\n",
    "    \n",
    "    # List to tensor\n",
    "    x_input_ids = torch.tensor(x_input_ids)\n",
    "    y = torch.tensor(y)\n",
    "    \n",
    "    tensor_dataset = TensorDataset(x_input_ids, y)\n",
    "    \n",
    "    # Fine Tunning(Training, Validation), Testing\n",
    "    # Fine tunning(Training ratio = 0.8, Validation = 0.2)\n",
    "    \n",
    "    if type == \"train\":\n",
    "        # Pemisahana data untuk training\n",
    "        train_tensor_dataset, valid_tensor_dataset = torch.utils.data.random_split(tensor_dataset, [\n",
    "                                                                              round(len(x_input_ids) * 0.8), \n",
    "                                                                              len(x_input_ids) - round(len(x_input_ids) * 0.8)])\n",
    "        torch.save(train_tensor_dataset, \"preprocessed/train.pt\")\n",
    "        torch.save(valid_tensor_dataset, \"preprocessed/valid.pt\")\n",
    "        \n",
    "        return train_tensor_dataset, valid_tensor_dataset\n",
    "    \n",
    "    else:\n",
    "        torch.save(tensor_dataset, \"preprocessed/test.pt\")\n",
    "        return tensor_dataset\n",
    "        \n",
    "\n",
    "train_dataset, validation_dataset = arange_data(data = train, type = \"train\")\n",
    "test_dataset = arange_data(data = test, type = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "def preprocessor(train_datasets, validation_datasets, test_datasets):\n",
    "    train_datasets = DataLoader(\n",
    "        dataset = train_datasets,\n",
    "        batch_size = batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers = 4\n",
    "    )\n",
    "    \n",
    "    validation_datasets = DataLoader(\n",
    "        dataset = validation_datasets,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = 4\n",
    "    )\n",
    "    \n",
    "    test_datasets = DataLoader(\n",
    "        dataset = test_datasets,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = 4 \n",
    "    )\n",
    "    \n",
    "    return train_datasets, validation_datasets, test_datasets\n",
    "\n",
    "train_datasets, validation_datasets, test_datasets = preprocessor(train_dataset, validation_dataset, test_dataset)\n",
    "train_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mulai Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_datasets:\n",
    "    x_input_ids, y = batch\n",
    "    print(x_input_ids.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a897da0134c4af538d8145dc3818d8156ca8f66c244eb9debe26b9ca43b0cc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
